{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.5.15-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ------------------ ------------------- 20.5/42.0 kB 682.7 kB/s eta 0:00:01\n",
      "     --------------------------- ---------- 30.7/42.0 kB 445.2 kB/s eta 0:00:01\n",
      "     -------------------------------------- 42.0/42.0 kB 406.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\envs\\novic\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\novic\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2024.5.15-cp312-cp312-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.5 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 61.4/268.5 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 92.2/268.5 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 204.8/268.5 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  266.2/268.5 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 268.5/268.5 kB 1.5 MB/s eta 0:00:00\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.8.1 regex-2024.5.15\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a naive bayes to classify the sentiment of the tweets\n",
    "\n",
    "# import the necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to C:\\Users\\Mosub\n",
      "[nltk_data]     Gamal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the positive and negative tweets from the twitter samples corpus\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mosub\n",
      "[nltk_data]     Gamal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove stop words\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to clean the tweets\n",
    "import re\n",
    "def clean_tweet(tweet):\n",
    "    # Convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    cleaned_tweet = [stemmer.stem(word) for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    cleaned_tweet = [word for word in cleaned_tweet if word not in stopwords]\n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show stop words\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dirty Tweet : hopeless for tmr :(\n",
      "clean tweet : ['hopeless', 'tmr']\n",
      "\n",
      "Dirty Tweet : Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\n",
      "clean tweet : ['everyth', 'kid', 'section', 'ikea', 'cute', 'shame', 'nearli', 'month']\n",
      "\n",
      "Dirty Tweet : @Hegelbon That heart sliding into the waste basket. :(\n",
      "clean tweet : ['hegelbon', 'heart', 'slide', 'wast', 'basket']\n",
      "\n",
      "Dirty Tweet : “@ketchBurning: I hate Japanese call him \"bani\" :( :(”\n",
      "\n",
      "Me too\n",
      "clean tweet : ['ketchburn', 'hate', 'japanes', 'call', 'bani']\n",
      "\n",
      "Dirty Tweet : Dang starting next week I have \"work\" :(\n",
      "clean tweet : ['dang', 'start', 'next', 'week', 'work']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Dirty Tweet :\", negative_tweets[i])\n",
    "    print(\"clean tweet :\", clean_tweet(negative_tweets[i]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'thank': 642, 'follow': 446, 'love': 399, 'thi': 304, 'u': 247, 'day': 242, 'good': 238, 'like': 232, 'happi': 211, 'get': 209, ...})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute frequency distribution of words for positive and negative tweets\n",
    "pos_freq = nltk.FreqDist([word for tweet in positive_tweets for word in clean_tweet(tweet)])\n",
    "pos_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'thi': 319, 'miss': 301, 'pleas': 275, 'wa': 263, 'follow': 263, 'want': 246, 'get': 233, 'go': 223, 'like': 223, 'u': 189, ...})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute frequency distribution of words for positive and negative tweets\n",
    "neg_freq = nltk.FreqDist([word for tweet in negative_tweets for word in clean_tweet(tweet)])\n",
    "neg_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>followfriday</td>\n",
       "      <td>25</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>top</td>\n",
       "      <td>31</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>engag</td>\n",
       "      <td>7</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>member</td>\n",
       "      <td>16</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>commun</td>\n",
       "      <td>33</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word  Frequency      Type\n",
       "0  followfriday         25  Positive\n",
       "1           top         31  Positive\n",
       "2         engag          7  Positive\n",
       "3        member         16  Positive\n",
       "4        commun         33  Positive"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert neg/pos freq dist to one pandas dataframe\n",
    "import pandas as pd\n",
    "pos_df = pd.DataFrame({'Word': list(pos_freq.keys()), 'Frequency': list(pos_freq.values())})\n",
    "pos_df['Type'] = 'Positive'\n",
    "neg_df = pd.DataFrame({'Word': list(neg_freq.keys()), 'Frequency': list(neg_freq.values())})\n",
    "neg_df['Type'] = 'Negative'\n",
    "# combine pos and neg dataframes\n",
    "tweets_df = pd.concat([pos_df, neg_df])\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Type</th>\n",
       "      <th>Word</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaaaaaaaaaa</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaaaaaaaaaaaa</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaaaaaaaaaaah</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaaaaand</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Type           Word  Positive  Negative\n",
       "0                aa       2.0       0.0\n",
       "1       aaaaaaaaaaa       0.0       1.0\n",
       "2     aaaaaaaaaaaaa       0.0       1.0\n",
       "3     aaaaaaaaaaaah       0.0       1.0\n",
       "4          aaaaaand       1.0       0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pivot table to show frequency of words across positive/negative tweets\n",
    "tweets_pivot = tweets_df.pivot(index='Word', columns='Type', values='Frequency').fillna(0)\n",
    "tweets_pivot.sort_values('Positive', ascending=False).head()\n",
    "\n",
    "# convert it to 3 by m frame\n",
    "tweets_pivot = tweets_pivot.reset_index()\n",
    "\n",
    "# # Remove Type column\n",
    "# tweets_pivot = tweets_pivot.drop('Type', axis=1)\n",
    "\n",
    "# Get only the last three columns\n",
    "tweets_pivot = tweets_pivot[['Word','Positive','Negative']]\n",
    "tweets_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\programdata\\anaconda3\\envs\\novic\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\programdata\\anaconda3\\envs\\novic\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Collecting matplotlib!=3.6.1,>=3.4 (from seaborn)\n",
      "  Downloading matplotlib-3.9.1-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading contourpy-1.2.1-cp312-cp312-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading fonttools-4.53.1-cp312-cp312-win_amd64.whl.metadata (165 kB)\n",
      "     ---------------------------------------- 0.0/165.9 kB ? eta -:--:--\n",
      "     -- ------------------------------------- 10.2/165.9 kB ? eta -:--:--\n",
      "     --------- --------------------------- 41.0/165.9 kB 388.9 kB/s eta 0:00:01\n",
      "     -------------------- ---------------- 92.2/165.9 kB 744.7 kB/s eta 0:00:01\n",
      "     ------------------------------------ 165.9/165.9 kB 905.1 kB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading kiwisolver-1.4.5-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\envs\\novic\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\envs\\novic\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.3.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\envs\\novic\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\envs\\novic\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\envs\\novic\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\envs\\novic\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading matplotlib-3.9.1-cp312-cp312-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/8.0 MB 2.2 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.2/8.0 MB 2.3 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/8.0 MB 2.5 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.5/8.0 MB 2.6 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.6/8.0 MB 2.4 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.7/8.0 MB 2.5 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.8/8.0 MB 2.5 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.9/8.0 MB 2.5 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.0/8.0 MB 2.5 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.1/8.0 MB 2.5 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.2/8.0 MB 2.5 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.4/8.0 MB 2.5 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.5/8.0 MB 2.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.6/8.0 MB 2.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.7/8.0 MB 2.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 1.8/8.0 MB 2.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 1.9/8.0 MB 2.4 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.0/8.0 MB 2.4 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.2/8.0 MB 2.4 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.3/8.0 MB 2.4 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.4/8.0 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.5/8.0 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.6/8.0 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.7/8.0 MB 2.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.8/8.0 MB 2.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.9/8.0 MB 2.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/8.0 MB 2.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.2/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.3/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.4/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.5/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.6/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.7/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.9/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.0/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.1/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.2/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.3/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.4/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.5/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.6/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.8/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.9/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.0/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.1/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.2/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.3/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.4/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.5/8.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.6/8.0 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.7/8.0 MB 2.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.8/8.0 MB 2.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.9/8.0 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.0/8.0 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.0/8.0 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.0/8.0 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.0/8.0 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.0/8.0 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.0/8.0 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.1/8.0 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.6/8.0 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.6/8.0 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.7/8.0 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.7/8.0 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.8/8.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.9/8.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.0/8.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/8.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/8.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.3/8.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.4/8.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.5/8.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.5/8.0 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.6/8.0 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.7/8.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.8/8.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/8.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 2.2 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.2.1-cp312-cp312-win_amd64.whl (189 kB)\n",
      "   ---------------------------------------- 0.0/189.9 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 30.7/189.9 kB 1.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 81.9/189.9 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 163.8/189.9 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 189.9/189.9 kB 1.4 MB/s eta 0:00:00\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.53.1-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.2 MB 991.0 kB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.1/2.2 MB 1.4 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.2 MB 1.6 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.4/2.2 MB 1.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.5/2.2 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.7/2.2 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.8/2.2 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.9/2.2 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.1/2.2 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.2/2.2 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.3/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.4/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.4/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.5/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.7/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.5-cp312-cp312-win_amd64.whl (56 kB)\n",
      "   ---------------------------------------- 0.0/56.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 56.0/56.0 kB 3.1 MB/s eta 0:00:00\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib, seaborn\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.53.1 kiwisolver-1.4.5 matplotlib-3.9.1 pyparsing-3.1.2 seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "# fix seaborn library\n",
    "! pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# plot word frequency distribution for positive and negative tweets\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# plot word frequency distribution for positive and negative tweets\n",
    "sns.barplot(x='Word', y='Frequency', hue='Type', data=tweets_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
